# Welcome to My GitHub Homepage üëã

I'm an engineer with a diverse technical background: embedded systems during undergrad, computer vision during graduate, and quantitative development in the finance industry. 
This space documents both recent and past explorations, aiming to revisit core ideas and extract deeper insights through hands-on experimentation. üöÄ

---

## ‚≠ê Jul‚ÄìAug 2025: Revisiting Transformer Details

- [Transformer Copy Task](https://github.com/PengTang2025/transformer_copy)  
  A basic implementation of the Transformer model with attention visualization support.

- [Transformer Attention Visualization - customized layer](https://github.com/PengTang2025/modified_transformer_to_visualize_attention)  
  Extracts multi-head attention weights from both the encoder and decoder, built by inheriting from PyTorch‚Äôs encoder and decoder layers to expose attention weights.

- [Transformer Attention Visualization Experiment ‚Äî Pig Latin Seq2Seq Task](https://github.com/PengTang2025/TransformerSeq2Seq-CopyTask-with-AttentionVis-CustomPigLatin)  
  A sequence-to-sequence task translating English to Pig Latin. Visualizations of various encoder and decoder attention patterns are generated using a semantically meaningful dataset.

---

## ‚≠ê Early Explorations

- to be organized...

---

## üóÇÔ∏è Project Index (by Tech Stack & Year)

| Tech Stack   | Project                               | Language/Tools    | Year  |
|--------------|----------------------------------------|-------------------|--------|
| Transformer  | Transformer Attention Visualization    | Python / PyTorch  | 2025   |
| Transformer  | Pig Latin Seq2Seq                     | Python / PyTorch  | 2025   |
| Transformer  | Transformer Copy Task                 | Python / PyTorch  | 2025   |

---  

## üíº About Me

- Languages: Python, C++
- Frameworks: PyTorch
- Interests: Computer vision and graph neural networks, with a current focus on Transformer models and attention mechanisms.
